{
  "posts": [
    {
      "id": "llm-power-rec",
      "title": "Personal Survey on LLM Power Rec",
      "date": "2025-08-22",
      "shortDate": "August 22, 2025",
      "intro": "A quick look at the three main ways LLMs are used in recommendation today. This survey explores how large language models are transforming recommendation systems through reranking, feature extraction, and explanation generation.",
      "content": [
        {
          "type": "category",
          "title": "LLM-enhanced rerank",
          "description": "First get a small list with CF, then ask an LLM to reorder using the user's request and item text. Good for tricky asks like \"cozy sci-fi under 2 hours.\""
        },
        {
          "type": "category",
          "title": "LLM as features",
          "description": "Let the LLM read text and output signals (e.g., \"family-friendly = yes/no\"). A smaller model uses these to rank items."
        },
        {
          "type": "category",
          "title": "RAG for explanation",
          "description": "Retrieve snippets from item pages or reviews, and have the LLM write short reasons. Ranking stays classic—the LLM just explains."
        },
        {
          "type": "rerank",
          "title": "Rerank",
          "description": "A two-pass setup: shortlist first, then reorder smartly.",
          "figure": "Figures/Blog - 20250822.png",
          "citation": "https://arxiv.org/abs/2504.07439?utm_source=chatgpt.com",
          "steps": [
            "First pass (candidate list): Use a fast ranking model (e.g., GMF/CF) with user info and the whole item set to get N candidates.",
            "Second pass (reranking): Feed that shortlist plus user info into a reranker (LLM or small model). It re-scores, reorders, and may drop items.",
            "Final result: Output the top K items (K ≤ N) as the recommendations, optionally with reasons."
          ]
        },
        {
          "type": "advanced",
          "title": "Advanced Work",
          "papers": [
            {
              "title": "LLM4Rerank (2024/25)",
              "description": "Multi-objective reranking with structured prompts for accuracy, diversity, fairness — rec-specific, not just IR.",
              "link": "https://arxiv.org/abs/2406.12433?utm_source=chatgpt.com"
            },
            {
              "title": "Prompt-Based LLMs for Position-Bias-Aware Reranking (2025)",
              "description": "Tests prompt tricks on MovieLens-100K; finds LLM rerank doesn't always beat strong baselines.",
              "link": "https://arxiv.org/abs/2505.04948?utm_source=chatgpt.com"
            },
            {
              "title": "LLM4Ranking (2025)",
              "description": "A standardized framework for LLM reranking pipelines and evaluation — aimed at practical tooling.",
              "link": "https://arxiv.org/abs/2504.07439?utm_source=chatgpt.com"
            },
            {
              "title": "CORANK (2025)",
              "description": "Compresses doc info into compact features so LLM rerank is cheaper and faster. Efficiency first.",
              "link": "https://arxiv.org/pdf/2505.13757?utm_source=chatgpt.com"
            },
            {
              "title": "Adaptive Repetition for Position Bias (2025)",
              "description": "Uses repeated listwise prompts with dynamic early-stopping to reduce per-query bias.",
              "link": "https://arxiv.org/abs/2507.17788?utm_source=chatgpt.com"
            }
          ]
        }
      ],
      "conclusion": "LLMs are reshaping recommendation systems through three main approaches: intelligent reranking, semantic feature extraction, and natural language explanations. While the field is rapidly evolving with new papers addressing efficiency, bias, and practical deployment, the core challenge remains balancing the power of LLMs with the computational and cost constraints of real-world systems."
    },
    {
      "id": "privacy-rag",
      "title": "Privacy in RAG: Risks and Fixes Along the Pipeline",
      "date": "2025-08-19",
      "shortDate": "August 19, 2025",
      "intro": "The RAG has been widely used in industry because it can keep databases internal and reduce mistakes. But the privacy problem still exists during the whole process. I conclude the different kinds of privacy risks and how we could fix them. Hope this provides some insights for future study.",
      "content": [
        {
          "type": "section",
          "title": "1. Database Stores Documents",
          "risk": "Sensitive data (PII, secrets, contracts) is stored. If attackers access the database or embeddings leak, they may recover the text.",
          "solution": "Encrypt storage, apply strict access control, avoid sharing embeddings, detect and remove PII before indexing.",
          "notFixed": "Embedding inversion — attackers can guess text from vectors. Preventing this without losing accuracy is still open."
        },
        {
          "type": "section",
          "title": "2. Retrieve Documents for a Query",
          "risk": "Wrong people may retrieve private docs (cross-user leakage).",
          "solution": "Use tenant separation and permission checks before retrieval.",
          "notFixed": "Membership inference — attackers probing to guess whether certain documents exist."
        },
        {
          "type": "section",
          "title": "3. Feed Retrieved Docs into LLM Prompt",
          "risk": [
            "Indirect prompt injection: Malicious text inside docs can trick the LLM (\"ignore rules,\" \"reveal password\").",
            "Sensitive leakage: The LLM may summarize or expose private details unintentionally."
          ],
          "solution": "Sanitize docs, scan for hidden instructions, use guardrail models (e.g., Prompt Guard, Llama Guard), filter PII before passing to LLM.",
          "notFixed": "No reliable way yet to fully block hidden instructions in retrieved text."
        },
        {
          "type": "section",
          "title": "4. LLM Generates the Final Answer",
          "risk": [
            "The LLM may output private info directly.",
            "It may combine different details into something more revealing."
          ],
          "solution": "Apply output filters (PII/secrets detection), limit context sharing, enforce policies on what can leave the system.",
          "notFixed": "Balancing helpfulness with not leaking too much. No formal guarantee yet."
        }
      ],
      "conclusion": "RAG improves reliability by grounding answers in real data, but privacy remains the weakest link. Every stage—storage, retrieval, prompting, generation—comes with its own risks. Fixes exist, but none are perfect. Bridging this gap between capability and privacy will be a key direction for future research."
    },
    {
      "id": "capability-reliability",
      "title": "Capability vs Reliability: The Real Challenge of LLM",
      "date": "2025-08-18",
      "shortDate": "August 18, 2025",
      "intro": "We are shocked by new developments day by day, and we all have the ambition to achieve big. But what really matters is reliability first. There are still a few challenges that bother people, and I hope to work on small pieces of these in the future.",
      "content": [
        {
          "type": "challenge",
          "title": "1. Hallucination",
          "description": "LLMs sometimes \"make things up\" with confidence. They don't really know facts; they just predict words.",
          "fix": "Retrieval-Augmented Generation (RAG), grounding models with external databases.",
          "remains": "Models still blend real facts with fiction. Better fact-checking and trust signals are needed."
        },
        {
          "type": "challenge",
          "title": "2. Ingratiation",
          "description": "Models tend to agree with users instead of correcting false assumptions.",
          "fix": "Reinforcement Learning with Human Feedback (RLHF) helps models learn when to push back.",
          "remains": "Hard to balance being polite with being accurate. Models often still \"echo\" user mistakes."
        },
        {
          "type": "challenge",
          "title": "3. Bias",
          "description": "LLMs carry human bias from training data—gender, culture, politics, etc.",
          "fix": "Data filtering, fairness constraints, bias audits.",
          "remains": "Subtle bias is hard to detect and often slips through. No universal measure of \"fairness\" yet."
        },
        {
          "type": "challenge",
          "title": "4. Privacy",
          "description": "Models risk leaking sensitive or private information.",
          "fix": "Differential Privacy, secure training pipelines, strict red-teaming.",
          "remains": "Trade-off between privacy and accuracy is unsolved, and enterprise data safety is still fragile."
        },
        {
          "type": "challenge",
          "title": "5. Cost and Energy",
          "description": "LLMs demand massive GPUs, electricity, and cooling.",
          "fix": "Model compression, distillation, efficient architectures (e.g., Mixture of Experts).",
          "remains": "Cost is still a barrier; running models sustainably and cheaply at scale is not solved."
        }
      ],
      "conclusion": "The impressive capability of LLMs often hides their reliability gap. If we want them to truly serve people, solving these challenges—step by step—will matter more than chasing size alone."
    }
  ]
}
